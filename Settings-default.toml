[web]
bind_address = "127.0.0.1:3030"
# set the path under which the api is served relative to the bind address
api_prefix = "/api"

# set the public address (typically ending in "/api") of the server, if it is different from the bind address
# external_address = "http://localhost:3030/api"

version_api = true

[project_service]
list_limit = 20

[dataset_service]
list_limit = 20

[layer_service]
list_limit = 20

[task_manager]
list_default_limit = 10
list_limit = 20

[postgres]
host = "localhost"
port = 5432
database = "geoengine"
schema = "public"
user = "geoengine"
password = "geoengine"
# Set to `true` to start with a fresh database each time.
# If this parameter is set to `false` at the first start, subsequent changes to `true` will result in an error.
# This helps to protect production environments from unwanted data loss.
clear_database_on_start = false

[raster.tiling_specification]
origin_coordinate_x = 0.0
origin_coordinate_y = 0.0
tile_shape_pixels_x = 512
tile_shape_pixels_y = 512

[query_context]
chunk_byte_size = 1048576 # TODO: find reasonable default

[upload]
path = "upload"

[logging]
# Minimum log level. Can be one of error, warn, info, debug, trace
# Console and file logging uses the tokio tracing crate.
# The spec defined here is parsed using the tracing_subscriber::EnvFilter. This allows to use complex filter expressions.
# For example `info,geoengine_services=debug` will log all info messages and all debug messages from the "geoengine_services" crate.
# To get all info messages, all debug messages from crates prefixed with "geoengine", and all trace messages from "geoengine_operators" use: `info,geoengine=debug,geoengine_operators=trace`.
# See https://docs.rs/tracing-subscriber/latest/tracing_subscriber/ for more information.
log_spec = "info"

# Whether the logs should be also written to files.
# Log files are rotated once per day. The last 7 files will be kept.
log_to_file = false

# Changes the first part of the log filename.
filename_prefix = "geo_engine"

# By default logs are saved in the current working directory.
# Use this option if another folder should be used.
#log_directory = "/var/log/"

[ogc.default_time]
# type "Value" with start/end as ISO strings or "Now" for using the current time, use wcs.default_time etc. for override
type = "Value"
start = "2014-01-01T00:00:00Z"
end = "2014-01-01T00:00:00Z"

[wcs]
# max number of tiles to be produced for generating output tiff
tile_limit = 4
# request_timeout_seconds = 3600

[wms]
# request_timeout_seconds = 3600

[wfs]
# request_timeout_seconds = 3600

[plots]
# request_timeout_seconds = 3600

[dataprovider]
dataset_defs_path = "./test_data/dataset_defs"
provider_defs_path = "./test_data/provider_defs"
layer_defs_path = "./test_data/layer_defs"
layer_collection_defs_path = "./test_data/layer_collection_defs"

[machinelearning]
model_defs_path = "./test_data/pro/ml"

[gdal]
# TODO: find good default
# Use 0 for `ALL_CPUS` option or a number >0 for a specific number of threads.
compression_num_threads = 8
compression_z_level = 6
compression_algorithm = "LZW"
allowed_drivers = [
    "COG",
    "CSV",
    "ESRI Shapefile",
    "FlatGeobuf",
    "GeoJSON",
    "GeoJSONSeq",
    "GPKG",
    "GTiff",
    "HDF4",
    "HDF5Image",
    "HTTP",
    "KML",
    "MEM",
    "Memory",
    "MSGN",
    "netCDF",
    "OGCAPI",
    "OSM",
    "PNG",
    "PostGISRaster",
    "PostgreSQL",
    "SAFE",
    "SQLite",
    "STACIT",
    "TopoJSON",
    "WCS",
    "WFS",
    "WMTS",
    "XLS",
    "XLSX",
    "Zarr",
    "GPX",
]

[session]
# Whether to allow requests to `/anonymous` that return a valid session.
anonymous_access = true

# Specifies a fixed session token. Otherwise, a random token is generated.
# This can be directly used for Bearer authentication in HTTP requests.
# This setting is ignored in the Pro version.
# fixed_session_token = "18fec623-6600-41af-b82b-24ccf47cb9f9"

# Settings for Geo Engine Pro
[user]
user_registration = true
admin_email = "admin@localhost"
admin_password = "adminadmin"   # Initial password for the user "admin", which is created on startup

[quota]
mode = "disabled" # "disabled", "track", "check"
# default_available_quota = 9999
increment_quota_buffer_size = 100           # number of quota updates to buffer before sending them to the database
increment_quota_buffer_timeout_seconds = 60 # number of seconds after which the quota updates are sent to the database

# Settings for compile-feature `nfdi` only
[gfbio]
# This is the basket API endpoint
basket_api_base_url = "https://dev.gfbio.uni-jena.de/daisi-api/api/baskets/"
# If this is set to true, basket entries that belong to the
# same dataset AND species are grouped within a single layer.
# This is the behaviour of the old VAT system.
# If false, every basket entry translates into a layer on the map.
group_abcd_units = true

[oidc]
enabled = false
issuer = ""
client_id = ""
redirect_uri = ""
scopes = []

[open_telemetry]
enabled = false
endpoint = "127.0.0.1:6831"

[data]
volumes = { test_data = "test_data/" }

[cache]
enabled = false
# storage limit for the cache
cache_size_in_mb = 1_000 # 1 GB
# storage limit for collecting query results before insertion into the cache in 
landing_zone_ratio = 0.1 # 10% of total cache size
